{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = '[Full_TFIDF]Credibility_research_20180906.db' #DB 파일명\n",
    "# db 생성\n",
    "con = sqlite3.connect( db_name )\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select p.Post_id,p.Category,p.Title,u.Credibility\n",
    "    FROM Post as p\n",
    "    Left JOIN user as u\n",
    "        ON p.User_id = u.User_id\n",
    "    '''\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = [i[0] for i in cur.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns = col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Post_id', 'Category', 'Title', 'Credibility']"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['Category'] != \"무작위\"].iloc[:16304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['Credibility']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_29 = pd.read_csv('[Final]Total_csv.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_29 = df_29[['Post_id','Question_count','First_ratio','Second_ratio','Tag_count','pos_ratio',\n",
    " 'neg_ratio','subjectivity','polarity','senti_diffs_per_ref','Sticker_count','Text_len','Count_space_mistake',\n",
    "   'Left','Center','Right','Justify',\n",
    "   'img img img img img','img img img img text','img img img text img','img img text img img','img img text img text',\n",
    " 'img text img img img','img text img img text','img text img text img','text img img img img','text img img img text',\n",
    " 'text img img text img','text img text img img','text img text img text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_29 features merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yonggeol/miniconda3/envs/py/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['Post_id'] = test_df['Post_id'].apply(lambda x : int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.merge(test_df,df_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(total_df['Category'])\n",
    "le.classes_\n",
    "total_df['Category'] =le.transform(total_df['Category'])\n",
    "total_df = total_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IT·컴퓨터', '건강·의학', '공연·전시', '교육·학문', '국내여행', '드라마·방송', '등산·낚시·레저',\n",
       "       '만화·애니', '맛집', '사진', '스포츠', '시사·인문·경제', '어학·외국어', '와인·술', '육아·결혼',\n",
       "       '자동차', '차·커피·디저트', '패션·뷰티'], dtype=object)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "total_df = shuffle(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = total_df['Title']\n",
    "y = total_df['Category']\n",
    "others = total_df.drop(columns=['Title','Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True,lowercase=True)\n",
    "vectorizer = TfidfVectorizer(max_df=0.01)\n",
    "tfidf = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(others) \n",
    "others_scaled = pd.DataFrame(scaler.transform(others),columns = others.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_result = pd.DataFrame(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat([tfidf_result,others],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf , y , test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6649, 30328), (6649,), (3275, 30328), (3275,))"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=42, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=2.0,random_state=42,solver='sag',multi_class='multinomial',warm_start=True)\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5233587786259541"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(logreg.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38922156, 0.315     , 0.49112426, 0.38285714, 0.20858896,\n",
       "       0.14772727, 0.26315789, 0.6713615 , 0.63421829, 0.93647913,\n",
       "       0.58088235, 0.25294118, 0.34      , 0.10526316, 0.04878049,\n",
       "       0.70833333, 0.19191919, 0.464     ])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(logreg.predict(x_test), y_test,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.39      0.54       167\n",
      "          1       0.93      0.32      0.47       200\n",
      "          2       0.94      0.49      0.65       169\n",
      "          3       0.97      0.38      0.55       175\n",
      "          4       0.71      0.21      0.32       163\n",
      "          5       1.00      0.15      0.26        88\n",
      "          6       0.87      0.26      0.40        76\n",
      "          7       0.95      0.67      0.79       213\n",
      "          8       0.72      0.63      0.68       339\n",
      "          9       0.27      0.94      0.42       551\n",
      "         10       0.88      0.58      0.70       272\n",
      "         11       0.81      0.25      0.39       170\n",
      "         12       1.00      0.34      0.51        50\n",
      "         13       1.00      0.11      0.19        19\n",
      "         14       1.00      0.05      0.09        82\n",
      "         15       0.94      0.71      0.81       192\n",
      "         16       0.95      0.19      0.32        99\n",
      "         17       0.86      0.46      0.60       250\n",
      "\n",
      "avg / total       0.78      0.52      0.54      3275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, logreg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 맛집 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textclass:\n",
    "    def Extract_structure_and_tag(User_id, Post_id):\n",
    "        url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "        r = requests.get(url)\n",
    "        bs = BeautifulSoup(re.sub('&nbsp;', ' ', r.text).encode(\"utf-8\"), \"html.parser\")\n",
    "        # structure\n",
    "        structure = bs.find(\"div\", {\"id\": \"postViewArea\"})\n",
    "        if structure == None:\n",
    "            structure = bs.find(\"div\", {\"class\", \"se_component_wrap sect_dsc __se_component_area\"})\n",
    "\n",
    "        structure_p_img_tag = structure.find_all(['p', 'img'])\n",
    "        structure_dict = {'structure': structure, 'structure_p_img_tag': structure_p_img_tag}\n",
    "        # structure_p_img_tag : p,img tag만 extract\n",
    "        # structure : 모든 tag 가져오기\n",
    "        return structure_dict\n",
    "\n",
    "    # Extract_structure_and_tag 함수의 'structure_p_img_tag' 값을 가져와야함.\n",
    "\n",
    "    def HTML_preprocessing(structure_p_img_tag):\n",
    "        # only tag & text extract\n",
    "        tag_list = []\n",
    "        text_list = []\n",
    "        for i in structure_p_img_tag:\n",
    "            # p_tag만 불러오기\n",
    "            if \"<p\" in (str(i)):\n",
    "                tag_list.append('<p>')\n",
    "                # img만 있을 때\n",
    "\n",
    "                if '<img' in str(i):\n",
    "                    for j in i:\n",
    "                        try:\n",
    "                            if len(j.text) > 1:\n",
    "                                tag_list.append('<br>')\n",
    "                                text_list.append(j.text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                # img가 아닌 경우 span tag가 더 있을 때\n",
    "                elif '<span' in str(i):\n",
    "                    for j in i:\n",
    "                        if '<br' in str(j):\n",
    "                            text_list.append(j.text)\n",
    "                            # br_tag가 2개 이상 있을 때\n",
    "\n",
    "                            if len(j.findAll('br')) > 2:\n",
    "                                for _ in range(0, len(j.findAll('br'))):\n",
    "                                    tag_list.append('<br>')\n",
    "\n",
    "                            # br_tag가 1개 있을 때\n",
    "                            else:\n",
    "                                tag_list.append('<br>')\n",
    "\n",
    "                        # span은 있지만 br tag가 없을 때\n",
    "                        else:\n",
    "                            try:\n",
    "                                text_list.append(j.text)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                # 그냥 p_tag만 있을 때 br_tag 추가\n",
    "                else:\n",
    "                    # 글이 있을 때\n",
    "                    if len(i.text) > 1:\n",
    "                        text_list.append(i.text)\n",
    "\n",
    "                    # 글 없이 br tag만 있을 때\n",
    "                    else:\n",
    "                        tag_list.append('<br>')\n",
    "                        text_list.append(i.text)\n",
    "\n",
    "                # P_tag 끝맽음\n",
    "                tag_list.append('</p>')\n",
    "\n",
    "            else:\n",
    "                tag_list.append('<img>')\n",
    "\n",
    "        text_list = list(map(remove_odd, text_list))\n",
    "        filter_text = list(filter(lambda x: len(x) > 1, text_list))\n",
    "\n",
    "        Text = \" \".join(list(filter(lambda x: len(x) > 1, map(lambda x: x.strip(), text_list))))\n",
    "        Text = re.sub('\\n', '', Text)\n",
    "        Text = re.sub('\\t', '', Text)\n",
    "        Space_text = \" \".join(Spacing_text(filter_text))\n",
    "        Count_space_mistake = len(Space_text) - len(Text)\n",
    "\n",
    "        # only tag\n",
    "        Structure_only_tag = \"|\".join(tag_list)\n",
    "        Structure_only_tag_df = pd.DataFrame({'text': [Structure_only_tag]})\n",
    "        array_temp = Structure_only_tag_df['text'].apply(\n",
    "            lambda x: \" img \".join(list(map(lambda x: 'text' if len(x) > 3 else '', x.split('<img>')))).strip().replace(\n",
    "                '  ', ' ')).values\n",
    "        refined_structure = ''.join(array_temp)\n",
    "\n",
    "        HTML_preprocessing = {'Text': Text, 'refined_structure': refined_structure,\n",
    "                              'Count_space_mistake': Count_space_mistake}\n",
    "\n",
    "        return HTML_preprocessing\n",
    "\n",
    "    def sentimental_analysis(text):\n",
    "        pos_word_list = []\n",
    "        neg_word_list = []\n",
    "        \n",
    "        pos_ratio = 0.000000001\n",
    "        neg_ratio = 0.000000001\n",
    "        subjectivity = 0.000000001\n",
    "        polarity = 0.000000001\n",
    "        senti_diffs_per_ref = 0.000000001\n",
    "\n",
    "        if text == '':\n",
    "            sentiment_dict = {'pos_ratio': pos_ratio, 'neg_ratio': neg_ratio, 'subjectivity': subjectivity,\n",
    "                              'polarity': polarity, 'senti_diffs_per_ref': senti_diffs_per_ref}\n",
    "            return sentiment_dict, pos_word_list, neg_word_list\n",
    "        else:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            text = text.split(' ')\n",
    "            n = len(text)\n",
    "            for i in text:\n",
    "                i = remove_odd(i)\n",
    "                pre = kkma.pos(i)\n",
    "                test = ';'.join(['/'.join(i) for i in pre])\n",
    "                if test in word_list:\n",
    "                    if label[word_list.index(test)] == 'POS':\n",
    "                        pos += 1\n",
    "                        pos_word_list.append(test)\n",
    "                    elif label[word_list.index(test)] == 'NEG':\n",
    "                        neg += 1\n",
    "                        neg_word_list.append(test)\n",
    "            try:\n",
    "                pos_ratio = pos / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                neg_ratio = neg / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                subjectivity = (neg + pos) / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                polarity = (neg - pos) / (neg + pos)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                senti_diffs_per_ref = (pos - neg) / n\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            sentiment_dict = {'pos_ratio': pos_ratio, 'neg_ratio': neg_ratio, 'subjectivity': subjectivity,\n",
    "                              'polarity': polarity, 'senti_diffs_per_ref': senti_diffs_per_ref}\n",
    "            return sentiment_dict, pos_word_list, neg_word_list\n",
    "\n",
    "    def check_First_second(Text):\n",
    "        first_person = ['나/NP', '저/NP', '내/NP', '제/NP', '저희/NP', '우리/NP']\n",
    "        second_person = ['너/NP', '자네/NP', '당신/NP', '그대/NP', '그쪽/NP', '너희/NP', '자기/NP']\n",
    "        First = 0\n",
    "        Second = 0\n",
    "        if Text == '':\n",
    "            check_First_second_dict = {'First': First, 'Second': Second}\n",
    "            return check_First_second_dict\n",
    "        else:\n",
    "            text = kkma.pos(Text)\n",
    "            for i in text:\n",
    "                temp = \"/\".join(i)\n",
    "                if temp in first_person:\n",
    "                    First += len(temp.split('/')[0])\n",
    "                if temp in second_person:\n",
    "                    Second += len(temp.split('/')[0])\n",
    "            check_First_second_dict = {'First_ratio': First/len(Text), 'Second_ratio': Second/len(Text)}\n",
    "            return check_First_second_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spacing_text(text_list):\n",
    "    spacing_list = []\n",
    "    for i in text_list:\n",
    "        if len(i) < 197:\n",
    "            spacing_list.append(spacing(i))\n",
    "        else:\n",
    "            iteration = int(len(i) / 197)\n",
    "            mod = len(i) % 197\n",
    "            start = 0\n",
    "            end = 197\n",
    "            check = 0\n",
    "            while True:\n",
    "                # 시행횟수 < 몫\n",
    "                if check < iteration:\n",
    "                    spacing_list.append(spacing(i[start:end]))\n",
    "                    start += 197\n",
    "                    end += 197\n",
    "                    check += 1\n",
    "                else:\n",
    "                    # 마지막 횟수 + 나머지 더 slice\n",
    "                    spacing_list.append(spacing(i[iteration * 197:(iteration * 197) + mod]))\n",
    "                    break\n",
    "    return spacing_list\n",
    "\n",
    "def remove_odd(x):\n",
    "    x = re.sub(\"nbsp\", \" \", x)\n",
    "    x = re.sub(\"\\xa0\", \"\", x)\n",
    "    x = re.sub(\"\\u200b\", \"\", x)\n",
    "    x = re.sub(\"\\n\", \"\", x)\n",
    "    x = re.sub(\"\\t\", \"\", x)\n",
    "    x = re.sub('   ', ' ', x)\n",
    "    return x\n",
    "\n",
    "def tfidf_vectorizer(Text):\n",
    "    try:\n",
    "        return v_load.transform([Text]).toarray().flatten()\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex1 https://blog.naver.com/you-n-mi?Redirect=Log&logNo=221352751222\n",
    "# ex2 https://blog.naver.com/sky_sea11?Redirect=Log&logNo=221216249472\n",
    "# ex3 https://blog.naver.com/jhforever48/221154182850\n",
    "# ex4 https://blog.naver.com/soundbross?Redirect=Log&logNo=221403690848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "User_id = ['you-n-mi','sky_sea11','jhforever48','soundbross','senti54','0105114a']\n",
    "Post_id = ['221352751222','221216249472','221154182850','221403690848','221401308842','221269157353']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from pykospacing import spacing\n",
    "import pandas as pd\n",
    "from konlpy.tag import Kkma\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for User_id,Post_id in list(zip(User_id, Post_id)):\n",
    "    url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "    mobile_url = \"http://m.blog.naver.com/PostView.nhn?blogId=\"+ User_id\n",
    "    opening_url = 'http://blog.naver.com/profile/intro.nhn?blogId='+ User_id\n",
    "    structure = textclass.Extract_structure_and_tag(User_id,Post_id)\n",
    "    all_tag = structure['structure']\n",
    "    p_img_tag = structure['structure_p_img_tag']\n",
    "    HTML_preprocessing = textclass.HTML_preprocessing(p_img_tag)\n",
    "    text = HTML_preprocessing['Text']\n",
    "    text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "r = requests.get(url)\n",
    "bs = BeautifulSoup(re.sub('&nbsp;', ' ', r.text).encode(\"utf-8\"), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = bs.find(\"div\", {\"id\": \"postViewArea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"wrap_tag\">\n",
       "<div class=\"_param(false|false|3)\" id=\"tagList_221269157353\" style=\"display:none;cursor:default;\">\n",
       "<strong class=\"blind\">태그</strong>\n",
       "</div>\n",
       "<div style=\"display:none\">\n",
       "<span class=\"wrap_input_text\">\n",
       "<input class=\"input_text pcol2 _tagInputArea\" title=\"태그를 입력하세요.\" type=\"text\" value=\"\">\n",
       "<i class=\"aline\"></i><i class=\"pcol2b\"></i>\n",
       "</input></span>\n",
       "<span class=\"wrap_btn\"><a class=\"btn pcol2 _cancelTag _returnFalse _param(221269157353)\" href=\"#\">취소<i class=\"aline\"></i></a>\n",
       "<a class=\"btn pcol2 _saveTag _rosRestrict _returnFalse _param(221269157353)\" href=\"#\">확인<i class=\"aline\"></i></a></span>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.find(\"div\", {\"class\": \"wrap_tag\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
